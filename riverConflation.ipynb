{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paired river network conflation\n",
    "\n",
    "Works on the theory that the defining property of any location in a river network is its catchment. Thus in another representation of the same river network the same 'river location' can be found by finding the most similar catchment.\n",
    "\n",
    "Testing has shown that this method works well except where the river network line work has a level of detail that is not well supported well supported by the catchment data. Even in these cases the method may still have some value.\n",
    "\n",
    "Please reference this publication whenever this code is used\n",
    "http://geomorphometry.org/system/files/Read2011geomorphometry.pdf\n",
    "Thanks\n",
    "\n",
    "Testing has shown that it runs in a reasonable time/RAM usage with large datasets (approx 100000 subcatchments). However testing has been limited so YMMV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# note for development\n",
    "# I start ipython notebook with the  --script switch so that i get an importable .py version of this notebook on every save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from shapely.prepared import prep\n",
    "from shapely.geometry import shape\n",
    "from shapely.geometry import mapping,Polygon,Point,LineString\n",
    "from shapely.ops import transform\n",
    "import fiona\n",
    "from rtree import index\n",
    "import networkx as nx\n",
    "import collections\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first step is to produce a networkx multiDiGraph from your source data.\n",
    "\n",
    "The code below does that specifically for the Australian geofabric dataset in ESRI geodatabase format\n",
    "\n",
    "(note the geodatabase has to be upgraded in ESRI software first so that gdal can read it).\n",
    "\n",
    "For other datasets it would need to be changed to produce the same output from the different inputs.\n",
    "\n",
    "This needs to be done for both the conflation source and destination networks\n",
    "\n",
    "the key thing is some sort of from and to node attribution to build the network structure between subcatchments. if this isnt available it can be generated from the coords of the stream links. Code for this not here but available on request.\n",
    "\n",
    "subcatchments and streams need a one to one relationship. streams without a catchment are (i think) fully handled. catchments without streams are ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_geofabric_catch_duplicates(DG):\n",
    "    '''deal with special case of duplicate catchment polygons in the geofabric\n",
    "    \n",
    "    Im not sure why the geofabric allows multiple stream features to \n",
    "    have a relationship to the same subcatchment.\n",
    "    Maybe it is ok and i just need to change my expected data model..... will have to think about that.\n",
    "    Its a bit inconvinient though. \n",
    "    Is there a good way to weed these out logically and consistently?\n",
    "    Here is my attempt. Based on the fact that they seem to occur only at flow splits.\n",
    "    '''\n",
    "    ts = nx.topological_sort(DG)\n",
    "    for n in ts:\n",
    "        new_set = set()\n",
    "        for f,t,k,data in DG.in_edges_iter(n,data=True,keys=True):\n",
    "            new_set.update([(data['cid'])])\n",
    "        for f,t,k,data in DG.out_edges_iter(n,data=True,keys=True):\n",
    "            if data['cid'] in new_set:\n",
    "                data['cid'] = None\n",
    "                data['subCatch'] = Polygon()\n",
    "    \n",
    "    #unfortunatly many cases still remain\n",
    "    #simple way of handling them is to make an arbitary choice\n",
    "    #first in keeps the catchment\n",
    "    #not ideal\n",
    "    ss = set()\n",
    "    for f,t,k,data in DG.edges_iter(data=True,keys=True):\n",
    "        if data['cid'] is not None:\n",
    "            if data['cid'] in ss:\n",
    "                print 'WARNING: duplicate catchment removed catchment id = ' + str(data['cid'])\n",
    "                data['cid'] = None\n",
    "                data['subCatch'] = Polygon()\n",
    "            #assert data['cid'] not in ss, 'duplicate catchment id ' + str(data['cid'])\n",
    "            ss.add(data['cid'])\n",
    "\n",
    "def read_geofabric_data(netGDB,bbox=None):\n",
    "    catch = {}\n",
    "    with fiona.open(netGDB, layer='AHGFCatchment') as c:\n",
    "        for feat in c.items(bbox=bbox):\n",
    "            geom = shape(feat[1]['geometry'])\n",
    "            cid = feat[1]['properties']['HydroID']\n",
    "            assert cid not in catch #shouldnt be duplicates \n",
    "            catch[cid] = geom\n",
    "    \n",
    "    DG=nx.MultiDiGraph()\n",
    "    with fiona.open(netGDB, layer='AHGFNetworkStream') as c:\n",
    "        for feat in c.items(bbox=bbox):\n",
    "            streamLink = shape(feat[1]['geometry'])\n",
    "             #for some reason these are coming in as multipart features with only one part - no need for this\n",
    "            assert streamLink.type == 'MultiLineString'\n",
    "            assert len(streamLink.geoms) == 1\n",
    "            streamLink = streamLink.geoms[0]\n",
    "            \n",
    "            ##remove this - just here for testing\n",
    "            #if streamLink.representative_point().y > -40.5: #tasmania for testing\n",
    "            #    continue\n",
    "            \n",
    "            sid = feat[1]['properties']['HydroID']\n",
    "            cid = feat[1]['properties']['DrainID']\n",
    "            fid = feat[1]['properties']['From_Node']\n",
    "            tid = feat[1]['properties']['To_Node']\n",
    "            subCatch = catch.get(cid,Polygon())\n",
    "            DG.add_edge(fid, tid, id=sid,cid=cid,subCatch=subCatch,stream=streamLink)\n",
    "            \n",
    "    return DG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def network_copy_offset(DG,distance=0.0):\n",
    "    '''copies a network and shifts the subcatchment and stream coordinates by a distance\n",
    "    \n",
    "    used for testing conflation simpily using only one network\n",
    "    Can be used to provide an understanding of the sensitivity of the network to positional error in features'''\n",
    "    DG2 = DG.copy()\n",
    "    if distance != 0:\n",
    "        for f,t,k,data in DG2.edges_iter(data=True,keys=True):\n",
    "            data['subCatch'] = transform(lambda x, y, z=None: (x+distance, y+distance), data['subCatch'])\n",
    "            data['stream'] =  transform(lambda x, y, z=None: (x+distance, y+distance), data['stream'])\n",
    "    return DG2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remember rtrees are not picklable - doh\n",
    "# TODO: work out the rtree bulk loading method - its quicker apparently\n",
    "def build_index(DG):\n",
    "    '''build spatial and other indexes'''\n",
    "    \n",
    "    index_generator = ((0, data['subCatch'].bounds,(f,t,k)) for f,t,k,data in DG.edges_iter(data=True,keys=True) if not data['subCatch'].is_empty)\n",
    "    return index.Index(index_generator)\n",
    "\n",
    "def build_index_slow(DG):\n",
    "    DG_idx = index.Index()\n",
    "    for f,t,k,data in DG.edges_iter(data=True,keys=True):\n",
    "        if not data['subCatch'].is_empty:\n",
    "            DG_idx.insert(0, data['subCatch'].bounds,obj=(f,t,k))\n",
    "    return DG_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def upstream_edge_set_old_method(DG):\n",
    "    '''build up a list of upstream edge ids as an attribute in the network\n",
    "    \n",
    "    WARNING: these can have a large memory footprint'''\n",
    "    ts = nx.topological_sort(DG)\n",
    "    for n in ts:\n",
    "        new_set = set()\n",
    "        for f,t,k,data in DG.in_edges_iter(n,data=True,keys=True):\n",
    "            new_set.update((data['ids']))\n",
    "        for f,t,k,data in DG.out_edges_iter(n,data=True,keys=True):\n",
    "            data['ids'] = new_set.union([(f,t,k)])\n",
    "\n",
    "def upstream_edge_set(DG):\n",
    "    '''build up a list of upstream edge ids as an attribute in the network\n",
    "    \n",
    "    WARNING: these can have a large memory footprint'''\n",
    "    for e,ids in upstream_edge_set_iter(DG):\n",
    "        DG.get_edge_data(*e)['ids'] = ids\n",
    "            \n",
    "def upstream_edge_set_iter(DG):\n",
    "    '''build up a list of upstream edge ids in the network\n",
    "    \n",
    "    returns ((f,t,k),ids)\n",
    "    where (f,t,k) identifies the edge\n",
    "    and ids is a set of edge identifiers in the the same style.\n",
    "    \n",
    "    avoids the large memory footprint of storing these\n",
    "    and is more efficent then generating them at random for edges\n",
    "    because it uses the network topology to avoid multiple network traversals.'''\n",
    "    temp_dict = {}\n",
    "    ts = nx.topological_sort(DG)\n",
    "    for n in ts:\n",
    "        new_set = set()\n",
    "        for f,t,k,data in DG.in_edges_iter(n,data=True,keys=True):\n",
    "            new_set.update(temp_dict[(f,t,k)])\n",
    "            del temp_dict[(f,t,k)]\n",
    "        for f,t,k,data in DG.out_edges_iter(n,data=True,keys=True):\n",
    "            ids = new_set.union([(f,t,k)])\n",
    "            temp_dict[(f,t,k)] = ids\n",
    "            yield ((f,t,k),ids)\n",
    "            \n",
    "def node_upstream_edge_set(DG):\n",
    "    '''build up a list of upstream edge ids in the destination network\n",
    "    \n",
    "    WARNING: these can have a large memory footprint'''\n",
    "    for n in DG:\n",
    "        new_set = set()\n",
    "        for f,t,k,data in DG.in_edges_iter(n,data=True,keys=True):\n",
    "            new_set.update((data['ids']))\n",
    "        DG.node[n]['ids'] = new_set\n",
    "        \n",
    "\n",
    "#not used at present but might come in handy\n",
    "def upstream_node_set(DG):\n",
    "    '''build up a list of upstream nodes in the destination network\n",
    "    \n",
    "    WARNING: these can have a large memory footprint'''\n",
    "    ts = nx.topological_sort(DG)\n",
    "    for n in ts:\n",
    "        new_set = set([n])\n",
    "        for n2 in DG.predecessors_iter(n):\n",
    "            new_set.update(DG.node[n2]['nids'])\n",
    "        DG.node[n]['nids'] = new_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sub_catch_area(DG):\n",
    "    '''calc subcatchment area'''\n",
    "    for f,t,k,data in DG.edges_iter(data=True,keys=True):\n",
    "        data['area'] = data['subCatch'].area\n",
    "        \n",
    "def catch_area_needs_ids(DG):\n",
    "    '''calc catchment area\n",
    "    \n",
    "    works with anabranching networks without double counting'''\n",
    "    sub_catch_area(DG)\n",
    "    for f,t,k,data in DG.edges_iter(data=True,keys=True):\n",
    "        data['catchArea'] = sum(DG.get_edge_data(*e)['area'] for e in data['ids'])\n",
    "        \n",
    "def catch_area_no_ids(DG):\n",
    "    '''calc catchment area\n",
    "    \n",
    "    a simpiler but slower variation on catch_area\n",
    "    doesnt need ids precomputed\n",
    "    works with anabranching networks without double counting'''\n",
    "    sub_catch_area(DG)\n",
    "    for e,ids in upstream_edge_set_iter(DG):\n",
    "        DG.get_edge_data(*e)['catchArea'] = sum(DG.get_edge_data(*e2)['area'] for e2 in ids)\n",
    "        \n",
    "def node_catch_area(DG):\n",
    "    '''calc catchment area to nodes\n",
    "    \n",
    "    works with anabranching networks without double counting'''\n",
    "    for n in DG:\n",
    "        DG.node[n]['catchArea'] = sum(DG.get_edge_data(*e)['area'] for e in DG.node[n]['ids'])\n",
    "\n",
    "\n",
    "\n",
    "def catch_area(DG):\n",
    "    '''calc catchment area\n",
    "    \n",
    "    works with anabranching networks without double counting'''\n",
    "    # does catch area for both nodes and edges\n",
    "    # about 3 times faster than other area calculations\n",
    "    #although a bit more complex to understand the code\n",
    "    #doesnt need 'ids' from upstream_edge_set\n",
    "    temp_dict = {}\n",
    "    ts = nx.topological_sort(DG)\n",
    "    for n in ts:\n",
    "        new_set = set()\n",
    "        for f,t,k,data in DG.in_edges_iter(n,data=True,keys=True):\n",
    "            new_set.update(temp_dict[(f,t,k)])\n",
    "            del temp_dict[(f,t,k)]\n",
    "            DG.node[n]['catchArea'] = sum(a for a,f,t,k in new_set)\n",
    "        for f,t,k,data in DG.out_edges_iter(n,data=True,keys=True):\n",
    "            e_set = new_set.union([(data['subCatch'].area,f,t,k)])\n",
    "            temp_dict[(f,t,k)] = e_set\n",
    "            data['catchArea'] = sum(a for a,f,t,k in e_set)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_overlaps(DG1,DG2,DG2_idx):\n",
    "    '''build up dictionary of overlapping areas between 2 graphs'''\n",
    "\n",
    "    for f,t,data in DG1.edges_iter(data=True):\n",
    "        geom = data['subCatch']\n",
    "        data['overlaps'] = {}\n",
    "        if not geom.is_empty:\n",
    "            prepGeom = prep(geom)\n",
    "            for e in DG2_idx.intersection(geom.bounds,objects='raw'):\n",
    "                nGeom = DG2.get_edge_data(*e)['subCatch']\n",
    "                #prepGeom = prep(geom) #im a little wary of prep i have had it grind things to a halt when reused a lot\n",
    "                if prepGeom.intersects(nGeom):\n",
    "                    area = geom.intersection(nGeom).area\n",
    "                    if area > 0.0:\n",
    "                        data['overlaps'][e] = area\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_full_overlaps(DG):\n",
    "    '''stores full overlaps for all edges\n",
    "    \n",
    "    use is not recomended due to excessive memory foot print\n",
    "    use full_overlaps to produce what you need when you need it\n",
    "    '''\n",
    "    for f,t,k,data in DG.edges_iter(data=True,keys=True):\n",
    "        data['fullOverlaps'] = full_overlaps(DG,data['ids'])\n",
    "        \n",
    "def full_overlaps(DG,edges):\n",
    "    '''aggregate a table of overlapping area for set of edges.\n",
    "    \n",
    "    Usually done for the entire catchment above and including an edge.\n",
    "    assumes build_overlaps has been run with DG as the first network\n",
    "    works with anabranching river networks (MultiGraphs) without double counting\n",
    "    '''\n",
    "    fullOverlaps = collections.defaultdict(float)\n",
    "    for e in edges:\n",
    "        for e2, overlapArea in DG.get_edge_data(*e)['overlaps'].iteritems():\n",
    "            fullOverlaps[e2] += overlapArea\n",
    "    return fullOverlaps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If speed or memory usage is a problem then there are a few changes that could be implemented\n",
    "\n",
    "The first speed up might be to split DG1 up into seperate basins if possible. These could be run in parallel without any code changes.\n",
    "\n",
    "While find all matches trys to be very efficent in a large network if you are only interested in a few catchment then you could write a similar routine that only ran for those catchments to save some time.\n",
    "\n",
    "The 2 slow points are build_overlaps and find_all_matches (including full_overlaps).\n",
    "- could look at using cython for these. Although in build_overlaps the 85% of the time is spent in shapely/geos doing intersections\n",
    "- The next speed step is that both build_overlaps and find_all_matches could be easily implemented in parallel. would be best to prune the memory foot print of the required objects before copying for other processes (or use shared mem).\n",
    "- One memory saving is that there is no need to keep the spatial features in memory after build_overlaps is finished (need to extract node locations from streams in DG1 for find_matches).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_all_matches(DG1,DG2,DG2_idx,searchRadius,sizeRatio=0,maxMatchKeep=1):\n",
    "    '''for each catchment in DG1 find list the best matches\n",
    "\n",
    "    see find_matches for more info\n",
    "    '''\n",
    "    matches = {}\n",
    "    #build ids on the fly with the iterator to save a lot of memory\n",
    "    for e,eids in upstream_edge_set_iter(DG1):\n",
    "        m = find_matches(e,eids,DG1,DG2,DG2_idx,searchRadius,sizeRatio,maxMatchKeep)\n",
    "        if m:\n",
    "            matches[e] = m\n",
    "    return matches\n",
    "\n",
    "def find_matches(e,eids,DG1,DG2,DG2_idx,searchRadius,sizeRatio=0,maxMatchKeep=1):\n",
    "    '''for a catchment in DG1 find list the best matches\n",
    "\n",
    "    searchRadius\n",
    "    Limits the search to subcatchments within this radius\n",
    "    \n",
    "    sizeRatio\n",
    "    limits search and results to pairs that are similar in size\n",
    "    ie a sizeRatio of 0.5 wont test a pair where one catchment is double the area of the other\n",
    "    the closer to 1 the more the closer in area the 2 catchments need to be.\n",
    "    Mostly a time saving tweak Used in conjunction with searchRadius allows a bigger search area without a big time penalty\n",
    "        \n",
    "    maxMatchKeep\n",
    "    limits the number of potential matches that are kept to a short list of n items.\n",
    "    The best items are chosen using a simple test of the quality of the match\n",
    "    This is used to reduce the memory footprint of the returned dictionary\n",
    "    defaults to 1, set to None to keep all\n",
    "    '''\n",
    "    data = DG1.get_edge_data(*e)\n",
    "    # remember dont keep fullOverlaps - they use a lot of RAM\n",
    "    fullOverlaps = full_overlaps(DG1,eids)\n",
    "    if len(fullOverlaps) == 0:\n",
    "        # not going to find anything so move on and save nothing\n",
    "        return\n",
    "    match = []\n",
    "    searchBounds = Point(data['stream'].coords[-1]).buffer(searchRadius).bounds\n",
    "    for e in DG2_idx.intersection(searchBounds,objects='raw'):\n",
    "        data2 = DG2.get_edge_data(*e)\n",
    "        if sizeRatio > min(data['catchArea'],data2['catchArea'])/max(data['catchArea'],data2['catchArea']):\n",
    "            continue\n",
    "        # for this pair of edges work out the area of the overlap (its a subset of full overlaps)\n",
    "        overlap = sum( fullOverlaps.get(e2,0) for e2 in data2['ids'])\n",
    "        if overlap > 0.0:\n",
    "            qual = (overlap+overlap)/(data['catchArea']+data2['catchArea'])\n",
    "            #quality score goes first so it is easy to sort by\n",
    "            match.append((qual,overlap,e))\n",
    "    if len(match) == 0:\n",
    "        # nothing overlapping within the search bounds. This bit of the network is vastly different\n",
    "        # dont save any result\n",
    "        return\n",
    "    #just keep the best few matches\n",
    "    match.sort(reverse=True)\n",
    "    return match[0:maxMatchKeep]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#untested - \n",
    "\n",
    "def find_all_node_matches(DG1,DG2,DG2_idx,searchRadius,maxMatchKeep=1):\n",
    "    '''same as find_all_matches but working to a node not a subcatchment\n",
    "    '''\n",
    "    matches = {}\n",
    "    for n in DG1:\n",
    "        nIDs = DG1.node[n]['ids']\n",
    "        fullOverlaps = full_overlaps(DG1,nIDs)\n",
    "        if len(fullOverlaps) == 0:\n",
    "            # not going to find anything so move on and save nothing\n",
    "            continue\n",
    "        match = []\n",
    "        for f,t,k,data in DG1.in_edges_iter(n,data=True,keys=True):\n",
    "            searchBounds = Point(data['stream'].coords[-1]).buffer(searchRadius).bounds\n",
    "        for f2,t2,k2 in DG2_idx.intersection(searchBounds,objects='raw'):\n",
    "            data2 = DG2.get_edge_data(f2,t2,k2)\n",
    "            # for this pair of edges work out the area of the overlap (its a subset of full overlaps)\n",
    "            overlap = sum( fullOverlaps.get(e2,0) for e2 in DG2.node[t2]['ids'])\n",
    "            if overlap > 0.0:\n",
    "                qual = (overlap+overlap)/(DG1.node[n]['catchArea']+DG2.node[t2]['catchArea'])\n",
    "                #quality score goes first so it is easy to sort by\n",
    "                match.append((qual,overlap,e))\n",
    "        if len(match) == 0:\n",
    "            # nothing overlapping within the search bounds. This bit of the network is vastly different\n",
    "            # dont save any result\n",
    "            continue\n",
    "        #just keep the best few matches\n",
    "        match.sort(reverse=True)\n",
    "        matches[n] = match[0:maxMatchKeep]\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def best_match(matches):\n",
    "    return [(k,l[0][2],l[0][0]) for k,l in matches.iteritems()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_debug_lines(DG1,DG2,best,fileName):\n",
    "    '''a simple output to show how each to node matches up\n",
    "    \n",
    "    handy for looking for errors in the conflation'''\n",
    "    schema = {\n",
    "        'geometry': 'LineString',\n",
    "        'properties': {'qual': 'float'},\n",
    "    }\n",
    "    with fiona.open(fileName, 'w', 'ESRI Shapefile', schema) as c:\n",
    "        for e1,e2,qual in best:\n",
    "            p1 = DG1.get_edge_data(*e1)['stream'].coords[-1]\n",
    "            p2 = DG2.get_edge_data(*e2)['stream'].coords[-1]\n",
    "            geom = LineString(LineString([p1, p2]))\n",
    "            c.write({\n",
    "                'geometry': mapping(geom),\n",
    "                'properties': {'qual': qual},\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: not finished the reimplementation\n",
    "\n",
    "TODO\n",
    "\n",
    "unify the matches at a confluence to one match. difference between these and subcatchment match is also useful.\n",
    "\n",
    "transfer/rewrite code that maps the conflation out along reaches and subcatchments\n",
    "\n",
    "transfer/rewrite code that highlights topology differences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#code to test the idea of bringing the conflation of catchments at confluence being conflated together.\n",
    "#one way of doing this is done with find_all_node_matches.\n",
    "#The way being tested here is to use the existing list of good conflations to look at all inflowing catchments\n",
    "#to a confluence together and pick the best one.\n",
    "#each catchment is given equal weighting (not area weighted!!)\n",
    "#seems to work quite well.\n",
    "def confluence_matches(DG1,matches):\n",
    "    #TODO: tidy up this code\n",
    "    node_matches = []\n",
    "    for n in DG1.nodes_iter():\n",
    "        in_matches = []\n",
    "        for f,t,k,data in DG1.in_edges_iter(n,data=True,keys=True):\n",
    "            if (f,t,k) in matches:\n",
    "                in_matches.append (matches[(f,t,k)])\n",
    "        if len(in_matches) ==0:\n",
    "            continue\n",
    "\n",
    "        #get a list of the potential toNodes in the second network\n",
    "        uniqueNode = set()\n",
    "        for m in in_matches:\n",
    "            for qual,overlap,e in m:\n",
    "                f2,t2,k2 = e\n",
    "                uniqueNode.add(t2)\n",
    "        n_dict = {}\n",
    "        nScore = []\n",
    "        for n2 in uniqueNode:\n",
    "            n_dict[n2]= []\n",
    "            for m in in_matches:\n",
    "                #sometimes a toNode appears in 2 potential conflation matches - pick the best one\n",
    "                ll = [(qual,overlap,e) for qual,overlap,e in m if e[1] == n2]\n",
    "                if len(ll) > 0:\n",
    "                    n_dict[n2].append (max(ll))\n",
    "\n",
    "            s = sum(qual for qual,overlap,e in n_dict[n2])\n",
    "            nScore.append((s/len(in_matches),n,n2))\n",
    "\n",
    "        #print n_dict[max(nScore)[1]]\n",
    "        node_matches.append(max(nScore))\n",
    "        #print max(nScore),n\n",
    "    return node_matches\n",
    "\n",
    "def write_debug_lines_confluence_matches(DG1,DG2,node_matches,fileName):\n",
    "    '''a simple output to show how each to node matches up\n",
    "    \n",
    "    handy for looking for errors in the conflation'''\n",
    "    schema = {\n",
    "        'geometry': 'LineString',\n",
    "        'properties': {'qual': 'float'},\n",
    "    }\n",
    "    with fiona.open(fileName, 'w', 'ESRI Shapefile', schema) as c:\n",
    "        for qual,n1,n2 in node_matches:\n",
    "            for f,t,k,data in DG1.in_edges(n1,data=True,keys=True)[0:1]:\n",
    "                p1 = data['stream'].coords[-1]\n",
    "            for f,t,k,data in DG2.in_edges(n2,data=True,keys=True)[0:1]:\n",
    "                p2 = data['stream'].coords[-1]\n",
    "            geom = LineString(LineString([p1, p2]))\n",
    "            c.write({\n",
    "                'geometry': mapping(geom),\n",
    "                'properties': {'qual': qual},\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_catch(DG,fileName):\n",
    "    schema = {\n",
    "        'geometry': 'Polygon',\n",
    "        'properties': {'id': 'int',\n",
    "                      'area': 'float'},\n",
    "    }\n",
    "    with fiona.open(fileName, 'w', 'ESRI Shapefile', schema) as c:\n",
    "        for f,t,data in DG.edges_iter(data=True):\n",
    "            if not data['subCatch'].is_empty:\n",
    "                c.write({\n",
    "                    'geometry': mapping(data['subCatch']),\n",
    "                    'properties': {'id': data['id'],\n",
    "                                    'area':data['catchArea']}\n",
    "                    })\n",
    "\n",
    "def write_stream(DG,fileName):\n",
    "    schema = {\n",
    "        'geometry': 'LineString',\n",
    "        'properties': {'id': 'int',\n",
    "                      'area': 'float'},\n",
    "    }\n",
    "    with fiona.open(fileName, 'w', 'ESRI Shapefile', schema) as c:\n",
    "        for f,t,data in DG.edges_iter(data=True):\n",
    "            if not data['stream'].is_empty:\n",
    "                c.write({\n",
    "                    'geometry': mapping(data['stream']),\n",
    "                    'properties': {'id': data['id'],\n",
    "                                   'area':data['catchArea']}\n",
    "                })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#some code used for error testing in development\n",
    "#assumes full overlap\n",
    "#will get some show up due to floating point issues\n",
    "def test_stuff(DG):\n",
    "    for f,t,data in DG.edges_iter(data=True):\n",
    "        area = data['subCatch'].area\n",
    "        area2 = 0.0\n",
    "        for k, v in data['overlaps'].iteritems():\n",
    "                area2 += v\n",
    "        if (abs(area - area2) > 0.000000000000000001):\n",
    "            print area,area2\n",
    "\n",
    "    #a test for errors\n",
    "    for f,t,data in DG.edges_iter(data=True):\n",
    "        area = data['catchArea']\n",
    "        area2 = 0.0\n",
    "        for k, v in data['fullOverlaps'].iteritems():\n",
    "                area2 += v\n",
    "        if (abs(area - area2) > 0.00000000000001):\n",
    "            print area,area2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
